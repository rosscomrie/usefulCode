{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb4cfaa8890404e85051a9fdeb4f7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "token = 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\n",
    "notebook_login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"This is a system prompt\"},\n",
    "           {\"role\": \"user\", \"content\": \"Make up a rap about spaghetti\"}]\n",
    "         \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "input_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class ChatPromptTemplate:\n",
    "    def __init__(\n",
    "            base_model: str, \n",
    "                use_instruct: bool = False\n",
    "                ):\n",
    "        self.model_name = base_model \n",
    "        self.use_instruct = use_instruct\n",
    "        self.tokenizer = self._call_tokenizer(self.model_name), use_instruct\n",
    "        \n",
    "    def _call_tokenizer(self):\n",
    "        tokenizer_name = self.model_name\n",
    "        if self.use_instruct is True:\n",
    "            tokenizer_name += \"-Instruct\"\n",
    "        return AutoTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "    def format_prompt(self,\n",
    "                      messages: dict | None = None,\n",
    "                      return_tokens: bool = False,\n",
    "                      add_generation_prompt: bool = False):\n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=return_tokens,\n",
    "            add_generation_prompt=add_generation_prompt\n",
    "            )\n",
    "        return formatted_prompt\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import Logger\n",
    "logger = Logger(initial_level=\"DEBUG\", module_name=\"Local Model\")\n",
    "logger.debug(input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class ChatPromptTemplate:\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_model: str,\n",
    "            use_instruct: bool = False\n",
    "                ):\n",
    "        self.model_name = base_model \n",
    "        self.use_instruct = use_instruct\n",
    "        self.tokenizer = self._call_tokenizer()\n",
    "        \n",
    "    def _call_tokenizer(self):\n",
    "        tokenizer_name = self.model_name\n",
    "        if self.use_instruct is True:\n",
    "            tokenizer_name += \"-Instruct\"\n",
    "        return AutoTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "    def format_prompt(self,\n",
    "                      messages: dict | None = None,\n",
    "                      return_tokens: bool = False,\n",
    "                      add_generation_prompt: bool = False):\n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=return_tokens,\n",
    "            add_generation_prompt=add_generation_prompt\n",
    "            )\n",
    "        return formatted_prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"This is a system prompt\"},\n",
    "           {\"role\": \"user\", \"content\": \"Make up a rap about spaghetti\"}]\n",
    "         \n",
    "\n",
    "PromptFormatter = ChatPromptTemplate(base_model=base_model,use_instruct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nThis is a system prompt<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nMake up a rap about spaghetti<|eot_id|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptFormatter.format_prompt(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List, Dict, Any\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "class ChatPromptTemplate:\n",
    "    \"\"\"A class for formatting chat prompts using transformer tokenizers.\n",
    "    \n",
    "    This class handles the creation and formatting of chat prompts using HuggingFace\n",
    "    tokenizers, with support for both standard and instruction-tuned models.\n",
    "    \n",
    "    Attributes:\n",
    "        model_name (str): Name or path of the base model\n",
    "        use_instruct (bool): Whether to use instruction-tuned version of the model\n",
    "        tokenizer (PreTrainedTokenizer): The loaded tokenizer instance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model: Union[str, Path],\n",
    "        use_instruct: bool = False,\n",
    "        **tokenizer_kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the ChatPromptTemplate.\n",
    "        \n",
    "        Args:\n",
    "            base_model: Name or path of the HuggingFace model to use\n",
    "            use_instruct: If True, appends '-Instruct' to model name\n",
    "            **tokenizer_kwargs: Additional kwargs to pass to AutoTokenizer.from_pretrained()\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If base_model is empty or invalid\n",
    "            OSError: If model cannot be loaded\n",
    "        \"\"\"\n",
    "        if not base_model:\n",
    "            raise ValueError(\"base_model must not be empty\")\n",
    "            \n",
    "        self.model_name = str(base_model)\n",
    "        self.use_instruct = use_instruct\n",
    "        self.tokenizer = self._load_tokenizer(tokenizer_kwargs)\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_kwargs: Dict[str, Any]) -> PreTrainedTokenizer:\n",
    "        \"\"\"Load the tokenizer for the specified model.\n",
    "        \n",
    "        Args:\n",
    "            tokenizer_kwargs: Additional arguments for tokenizer initialization\n",
    "            \n",
    "        Returns:\n",
    "            The loaded tokenizer instance\n",
    "            \n",
    "        Raises:\n",
    "            OSError: If tokenizer cannot be loaded\n",
    "        \"\"\"\n",
    "        tokenizer_name = f\"{self.model_name}-Instruct\" if self.use_instruct else self.model_name\n",
    "        \n",
    "        try:\n",
    "            return AutoTokenizer.from_pretrained(tokenizer_name, **tokenizer_kwargs)\n",
    "        except Exception as e:\n",
    "            raise OSError(f\"Failed to load tokenizer '{tokenizer_name}': {str(e)}\")\n",
    "\n",
    "    def format_prompt(\n",
    "        self,\n",
    "        messages: Optional[List[Dict[str, str]]] = None,\n",
    "        return_tokens: bool = False,\n",
    "        add_generation_prompt: bool = False\n",
    "    ) -> Union[str, List[int]]:\n",
    "        \"\"\"Format chat messages into a prompt using the tokenizer's chat template.\n",
    "        \n",
    "        Args:\n",
    "            messages: List of message dictionaries with role and content\n",
    "            return_tokens: If True, returns tokenized output instead of string\n",
    "            add_generation_prompt: Whether to add a generation prompt\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt as either string or token ids\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If messages is None or empty\n",
    "            ValueError: If messages format is invalid\n",
    "        \"\"\"\n",
    "        if not messages:\n",
    "            raise ValueError(\"messages must not be None or empty\")\n",
    "            \n",
    "        if not all(isinstance(m, dict) and 'role' in m and 'content' in m for m in messages):\n",
    "            raise ValueError(\"Each message must be a dict with 'role' and 'content' keys\")\n",
    "            \n",
    "        try:\n",
    "            return self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=return_tokens,\n",
    "                add_generation_prompt=add_generation_prompt\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to format prompt: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Text:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Make up a rap about spaghetti<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Formatted Tokens:\n",
      "[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 2675, 527, 264, 11190, 15592, 18328, 13, 128009, 128006, 882, 128007, 271, 8238, 709, 264, 7477, 922, 88010, 128009, 128006, 78191, 128007, 271]\n",
      "\n",
      "Complex Conversation Format:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Make up a rap about spaghetti<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here's a rap about spaghetti:<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Make it longer<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Complex Conversation Format:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Make up a rap about spaghetti<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here's a rap about spaghetti:<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Make it longer<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Example usage\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the formatter\n",
    "base_model = \"meta-llama/Llama-3.1-8B\"  # Example model name\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Make up a rap about spaghetti\"},\n",
    "]\n",
    "\n",
    "# Basic usage\n",
    "formatter = ChatPromptTemplate(base_model=base_model, use_instruct=True)\n",
    "\n",
    "# Get formatted prompt as string\n",
    "formatted_text = formatter.format_prompt(\n",
    "    messages=messages,\n",
    "    return_tokens=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(\"Formatted Text:\")\n",
    "print(formatted_text)\n",
    "\n",
    "# Get formatted prompt as tokens\n",
    "formatted_tokens = formatter.format_prompt(\n",
    "    messages=messages,\n",
    "    return_tokens=True,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(\"\\nFormatted Tokens:\")\n",
    "print(formatted_tokens)\n",
    "\n",
    "# Using with additional tokenizer kwargs\n",
    "formatter_with_kwargs = ChatPromptTemplate(\n",
    "    base_model=base_model,\n",
    "    use_instruct=True,\n",
    "    trust_remote_code=True,  # Additional tokenizer kwarg\n",
    "    padding=True             # Additional tokenizer kwarg\n",
    ")\n",
    "\n",
    "# Example with more complex conversation\n",
    "complex_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Make up a rap about spaghetti\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Here's a rap about spaghetti:\"},\n",
    "    {\"role\": \"user\", \"content\": \"Make it longer\"},\n",
    "]\n",
    "\n",
    "formatted_complex = formatter.format_prompt(\n",
    "    messages=complex_messages,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(\"\\nComplex Conversation Format:\")\n",
    "print(formatted_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
